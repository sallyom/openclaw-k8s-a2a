# Deployment: openclaw
apiVersion: apps/v1
kind: Deployment
metadata:
  name: openclaw
  namespace: openclaw
  labels:
    app: openclaw
    kagenti.io/type: agent
    kagenti.io/protocol: a2a
    kagenti.io/framework: "OpenClaw"
    app.kubernetes.io/name: openclaw
    app.kubernetes.io/component: agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: openclaw
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: openclaw
        kagenti.io/type: agent
        kagenti.io/protocol: a2a
        kagenti.io/inject: disabled
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '18789'
        sidecar.opentelemetry.io/inject: "openclaw-sidecar"
        kagenti.io/description: "OpenClaw AI Agent Gateway"
    spec:
      serviceAccountName: openclaw-oauth-proxy
      initContainers:
      # AuthBridge: iptables redirect for transparent Envoy interception
      # Defined manually (not webhook-injected) so we control port exclusions.
      # Webhook's proxy-init ignores OUTBOUND_PORTS_EXCLUDE overrides on OpenShift.
      - name: proxy-init
        image: ghcr.io/kagenti/kagenti-extensions/proxy-init:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: PROXY_PORT
          value: "15123"
        - name: INBOUND_PROXY_PORT
          value: "15124"
        - name: PROXY_UID
          value: "1337"
        - name: OUTBOUND_PORTS_EXCLUDE
          value: "443,4317,4318,18789"
        - name: INBOUND_PORTS_EXCLUDE
          value: "8080,8443,18789,18790"
        resources:
          requests:
            cpu: 10m
            memory: 10Mi
          limits:
            cpu: 10m
            memory: 10Mi
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          capabilities:
            add:
            - NET_ADMIN
            - NET_RAW
          seLinuxOptions:
            type: spc_t
      - name: init-config
        image: registry.redhat.io/ubi9-minimal:latest
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            memory: 64Mi
            cpu: 50m
          limits:
            memory: 128Mi
            cpu: 200m
        command:
        - sh
        - -c
        - |
          # Copy main config
          cp /config/openclaw.json /home/node/.openclaw/openclaw.json
          chmod 644 /home/node/.openclaw/openclaw.json

          # Create OpenClaw default workspace structure
          mkdir -p /home/node/.openclaw/workspace
          mkdir -p /home/node/.openclaw/skills

          # Create agents directory structure for session transcripts and usage tracking
          # Session transcripts are stored at: ~/.openclaw/agents/<agent_id>/sessions/*.jsonl
          mkdir -p /home/node/.openclaw/agents
          mkdir -p /home/node/.openclaw/agents/shadowman/sessions
          mkdir -p /home/node/.openclaw/agents/shadowman/agent

          # Dynamically create session directories for all agents defined in config
          # This handles prefixed agent names (e.g., sallyom_resource_optimizer) without hardcoding
          grep -o '"id": *"[^"]*"' /config/openclaw.json | sed 's/"id": *"//;s/"$//' | while read agent_id; do
            mkdir -p "/home/node/.openclaw/agents/$agent_id/sessions"
          done

          # Copy shadowman agent config only on first init (don't overwrite customizations from setup-agents.sh)
          if [ ! -f /home/node/.openclaw/workspace/AGENTS.md ]; then
            cp /agents/shadowman/AGENTS.md /home/node/.openclaw/workspace/AGENTS.md 2>/dev/null || echo "Warning: shadowman AGENTS.md not found"
            cp /agents/shadowman/agent.json /home/node/.openclaw/workspace/agent.json 2>/dev/null || echo "Warning: shadowman agent.json not found"
          fi

          # Set permissions (for arbitrary UID in OpenShift)
          chmod -R 775 /home/node/.openclaw

          echo "âœ… Basic config initialized with shadowman agent"
          echo "   ðŸ“Š Session transcripts and usage data will be stored in agents/<agent_id>/sessions/"
          echo "   Run agent-setup-job.yaml to add more agents and cron jobs"
          ls -la /home/node/.openclaw/
          ls -la /home/node/.openclaw/workspace/ || true
          ls -la /home/node/.openclaw/agents/ || true
        volumeMounts:
        - name: openclaw-home
          mountPath: /home/node/.openclaw
        - name: config-template
          mountPath: /config
        - name: shadowman-agent
          mountPath: /agents/shadowman
      containers:
      - name: oauth-proxy
        image: quay.io/openshift/origin-oauth-proxy:4.14
        args:
        - --http-address=:8443
        - --https-address=
        - --provider=openshift
        - --upstream=http://localhost:18789
        - --client-id=openclaw
        - --client-secret-file=/etc/oauth/config/client-secret
        - --cookie-secret-file=/etc/oauth/config/cookie_secret
        - --cookie-expire=23h0m0s
        - --pass-access-token
        - --scope=user:full
        - --openshift-delegate-urls={"/":{"resource":"projects","verb":"list"}}
        - --skip-auth-regex=^/(metrics|api)
        ports:
        - containerPort: 8443
          name: oauth-ui
        volumeMounts:
        - name: oauth-config
          mountPath: /etc/oauth/config
        - name: proxy-tls
          mountPath: /etc/tls/private
        livenessProbe:
          httpGet:
            path: /oauth/healthz
            port: oauth-ui
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /oauth/healthz
            port: oauth-ui
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            memory: 64Mi
            cpu: 50m
          limits:
            memory: 128Mi
            cpu: 200m
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      - name: gateway
        image: quay.io/sallyom/openclaw:latest
        imagePullPolicy: Always
        command:
        - node
        - /app/dist/index.js
        - gateway
        - run
        - --bind
        - loopback
        - --port
        - '18789'
        - --verbose
        ports:
        - name: gateway
          containerPort: 18789
          protocol: TCP
        - name: bridge
          containerPort: 18790
          protocol: TCP
        env:
        - name: HOME
          value: /home/node
        - name: OPENCLAW_CONFIG_DIR
          value: /home/node/.openclaw
        - name: OPENCLAW_CLI_PATH
          value: /app/dist/index.js
        - name: TERM
          value: xterm-256color
        - name: NODE_ENV
          value: production
        - name: OPENCLAW_GATEWAY_TOKEN
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: OPENCLAW_GATEWAY_TOKEN
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: ANTHROPIC_API_KEY
              optional: true
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: OTEL_EXPORTER_OTLP_ENDPOINT
              optional: true
        # Telegram bot token (optional â€” only needed for Telegram channel)
        - name: TELEGRAM_BOT_TOKEN
          valueFrom:
            secretKeyRef:
              name: telegram-secrets
              key: bot-token
              optional: true
        # Google Vertex AI (for Gemini or Anthropic models via GCP)
        - name: GOOGLE_APPLICATION_CREDENTIALS
          value: /vertex/ambient-code-key.json
        - name: GCE_METADATA_HOST
          value: disabled
        - name: GOOGLE_CLOUD_PROJECT
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: GOOGLE_CLOUD_PROJECT
              optional: true
        - name: GOOGLE_CLOUD_LOCATION
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: GOOGLE_CLOUD_LOCATION
              optional: true
        - name: OTEL_SERVICE_NAME
          value: openclaw
        - name: OTEL_EXPORTER_OTLP_PROTOCOL
          value: http/protobuf
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: K8S_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        resources:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 2Gi
            cpu: 1000m
        livenessProbe:
          exec:
            command: ["node", "-e", "require('http').get('http://127.0.0.1:18789/', r => process.exit(r.statusCode < 400 ? 0 : 1)).on('error', () => process.exit(1))"]
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          exec:
            command: ["node", "-e", "require('http').get('http://127.0.0.1:18789/', r => process.exit(r.statusCode < 400 ? 0 : 1)).on('error', () => process.exit(1))"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 2
        volumeMounts:
        - name: openclaw-home
          mountPath: /home/node/.openclaw
        - name: tmp-volume
          mountPath: /tmp
        - name: vertex-credentials
          mountPath: /vertex
          readOnly: true
        - name: resource-report
          mountPath: /data/reports/resource-optimizer
          readOnly: true
        - name: mlops-report
          mountPath: /data/reports/mlops-monitor
          readOnly: true
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      # Agent card server â€” serves /.well-known/agent.json for Kagenti operator discovery.
      # Temporary until Kagenti webhook injects this or operator reads from CR directly.
      - name: agent-card
        image: registry.redhat.io/ubi9:latest
        command: ["python3", "-m", "http.server", "8080", "--directory", "/srv"]
        ports:
        - name: a2a
          containerPort: 8080
        volumeMounts:
        - name: agent-card-data
          mountPath: /srv/.well-known
        resources:
          requests:
            memory: 32Mi
            cpu: 10m
          limits:
            memory: 64Mi
            cpu: 50m
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      # AuthBridge: SPIFFE Helper â€” fetches SVIDs from SPIRE agent
      - name: spiffe-helper
        image: ghcr.io/spiffe/spiffe-helper:nightly
        imagePullPolicy: IfNotPresent
        command:
        - /spiffe-helper
        - -config=/etc/spiffe-helper/helper.conf
        - run
        volumeMounts:
        - name: spiffe-helper-config
          mountPath: /etc/spiffe-helper
        - name: spire-agent-socket
          mountPath: /run/spire/agent-sockets
          readOnly: true
        - name: svid-output
          mountPath: /opt
        resources:
          requests:
            cpu: 25m
            memory: 32Mi
          limits:
            cpu: 100m
            memory: 64Mi
        securityContext:
          seLinuxOptions:
            type: spc_t
      # AuthBridge: Client Registration â€” registers with Keycloak using SPIFFE ID
      - name: client-registration
        image: ghcr.io/kagenti/kagenti-extensions/client-registration:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/sh
        - -c
        - |
          echo "Waiting for SPIFFE credentials..."
          while [ ! -f /opt/jwt_svid.token ]; do
            echo "Waiting for SVID..."
            sleep 2
          done
          echo "SPIFFE credentials ready!"
          JWT_PAYLOAD=$(cat /opt/jwt_svid.token | cut -d'.' -f2)
          CLIENT_ID=$(echo "$JWT_PAYLOAD"== | base64 -d 2>/dev/null | python -c "import sys,json; print(json.load(sys.stdin).get('sub',''))")
          echo "$CLIENT_ID" > /shared/client-id.txt
          echo "Client ID (SPIFFE ID): $CLIENT_ID"
          echo "Starting client registration..."
          python client_registration.py
          echo "Client registration complete!"
          echo "Staying alive to keep pod running..."
          tail -f /dev/null
        env:
        - name: SPIRE_ENABLED
          valueFrom:
            configMapKeyRef:
              key: SPIRE_ENABLED
              name: environments
        - name: KEYCLOAK_URL
          valueFrom:
            configMapKeyRef:
              key: KEYCLOAK_URL
              name: environments
        - name: KEYCLOAK_REALM
          valueFrom:
            configMapKeyRef:
              key: KEYCLOAK_REALM
              name: environments
        - name: KEYCLOAK_ADMIN_USERNAME
          valueFrom:
            configMapKeyRef:
              key: KEYCLOAK_ADMIN_USERNAME
              name: environments
        - name: KEYCLOAK_ADMIN_PASSWORD
          valueFrom:
            configMapKeyRef:
              key: KEYCLOAK_ADMIN_PASSWORD
              name: environments
        - name: KEYCLOAK_TOKEN_EXCHANGE_ENABLED
          valueFrom:
            configMapKeyRef:
              key: KEYCLOAK_TOKEN_EXCHANGE_ENABLED
              name: environments
        - name: KEYCLOAK_CLIENT_REGISTRATION_ENABLED
          valueFrom:
            configMapKeyRef:
              key: KEYCLOAK_CLIENT_REGISTRATION_ENABLED
              name: environments
        - name: CLIENT_NAME
          value: openclaw
        - name: SECRET_FILE_PATH
          value: /shared/client-secret.txt
        volumeMounts:
        - name: shared-data
          mountPath: /shared
        - name: svid-output
          mountPath: /opt
        resources:
          requests:
            cpu: 25m
            memory: 32Mi
          limits:
            cpu: 100m
            memory: 128Mi
        securityContext:
          seLinuxOptions:
            type: spc_t
      # AuthBridge: Envoy Proxy â€” transparent token exchange
      - name: envoy-proxy
        image: ghcr.io/kagenti/kagenti-extensions/envoy-with-processor:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: TOKEN_URL
          valueFrom:
            configMapKeyRef:
              key: TOKEN_URL
              name: authbridge-config
        - name: ISSUER
          valueFrom:
            configMapKeyRef:
              key: ISSUER
              name: authbridge-config
        - name: TARGET_AUDIENCE
          valueFrom:
            configMapKeyRef:
              key: TARGET_AUDIENCE
              name: authbridge-config
        - name: TARGET_SCOPES
          valueFrom:
            configMapKeyRef:
              key: TARGET_SCOPES
              name: authbridge-config
        - name: CLIENT_ID_FILE
          value: /shared/client-id.txt
        - name: CLIENT_SECRET_FILE
          value: /shared/client-secret.txt
        ports:
        - containerPort: 15123
          name: envoy-outbound
          protocol: TCP
        - containerPort: 15124
          name: envoy-inbound
          protocol: TCP
        - containerPort: 9901
          name: envoy-admin
          protocol: TCP
        - containerPort: 9090
          name: ext-proc
          protocol: TCP
        volumeMounts:
        - name: envoy-config
          mountPath: /etc/envoy
          readOnly: true
        - name: shared-data
          mountPath: /shared
          readOnly: true
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 256Mi
        securityContext:
          runAsUser: 1337
          runAsGroup: 1337
          seLinuxOptions:
            type: spc_t
      volumes:
      - name: tmp-volume
        emptyDir: {}
      # AuthBridge volumes
      - name: shared-data
        emptyDir: {}
      - name: svid-output
        emptyDir: {}
      - name: spire-agent-socket
        csi:
          driver: csi.spiffe.io
          readOnly: true
      - name: spiffe-helper-config
        configMap:
          name: spiffe-helper-config
      - name: envoy-config
        configMap:
          name: envoy-config
      - name: oauth-config
        secret:
          secretName: openclaw-oauth-config
      - name: proxy-tls
        secret:
          secretName: openclaw-proxy-tls
      - name: openclaw-home
        persistentVolumeClaim:
          claimName: openclaw-home-pvc
      - name: config-template
        configMap:
          name: openclaw-config
      # Google Vertex AI credentials (optional â€” only needed for google-vertex provider)
      - name: vertex-credentials
        secret:
          secretName: ambient-vertex
          optional: true
      # Shadowman agent (included in core deployment)
      - name: shadowman-agent
        configMap:
          name: shadowman-agent
          optional: true
      # Resource report from K8s CronJob (updated every 8 hours)
      - name: resource-report
        configMap:
          name: resource-report-latest
          optional: true
      # MLOps report from K8s CronJob (updated every 6 hours)
      - name: mlops-report
        configMap:
          name: mlops-report-latest
          optional: true
      # Agent card for Kagenti operator discovery (/.well-known/agent.json)
      - name: agent-card-data
        configMap:
          name: openclaw-agent-card
      # Additional agents can be set up via setup-agents.sh
      # All agent workspaces are at ~/.openclaw/workspace* (inside openclaw-home-pvc)
