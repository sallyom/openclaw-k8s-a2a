# Deployment: openclaw
apiVersion: apps/v1
kind: Deployment
metadata:
  name: openclaw
  namespace: openclaw
  labels:
    app: openclaw
    kagenti.io/type: agent
    kagenti.io/protocol: a2a
    kagenti.io/framework: "OpenClaw"
    app.kubernetes.io/name: openclaw
    app.kubernetes.io/component: agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: openclaw
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: openclaw
        kagenti.io/type: agent
        kagenti.io/protocol: a2a
        kagenti.io/inject: enabled
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '18789'
        sidecar.opentelemetry.io/inject: "openclaw-sidecar"
        kagenti.io/description: "OpenClaw AI Agent Gateway"
        kagenti.io/outbound-ports-exclude: "443,4317,4318,18789"
        kagenti.io/inbound-ports-exclude: "8080,8443,18789,18790"
    spec:
      serviceAccountName: openclaw-oauth-proxy
      initContainers:
      # proxy-init is injected by the Kagenti webhook with port exclusions
      # from kagenti.io/outbound-ports-exclude and kagenti.io/inbound-ports-exclude annotations.
      - name: init-config
        image: registry.redhat.io/ubi9-minimal:latest
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            memory: 64Mi
            cpu: 50m
          limits:
            memory: 128Mi
            cpu: 200m
        command:
        - sh
        - -c
        - |
          # Copy main config
          cp /config/openclaw.json /home/node/.openclaw/openclaw.json
          chmod 644 /home/node/.openclaw/openclaw.json

          # Create OpenClaw default workspace structure
          mkdir -p /home/node/.openclaw/workspace
          mkdir -p /home/node/.openclaw/skills

          # Create agents directory structure for session transcripts and usage tracking
          # Session transcripts are stored at: ~/.openclaw/agents/<agent_id>/sessions/*.jsonl
          mkdir -p /home/node/.openclaw/agents
          mkdir -p /home/node/.openclaw/agents/shadowman/sessions
          mkdir -p /home/node/.openclaw/agents/shadowman/agent

          # Dynamically create session directories for all agents defined in config
          # This handles prefixed agent names (e.g., sallyom_resource_optimizer) without hardcoding
          grep -o '"id": *"[^"]*"' /config/openclaw.json | sed 's/"id": *"//;s/"$//' | while read agent_id; do
            mkdir -p "/home/node/.openclaw/agents/$agent_id/sessions"
          done

          # Copy shadowman agent config only on first init (don't overwrite customizations from setup-agents.sh)
          if [ ! -f /home/node/.openclaw/workspace/AGENTS.md ]; then
            cp /agents/shadowman/AGENTS.md /home/node/.openclaw/workspace/AGENTS.md 2>/dev/null || echo "Warning: shadowman AGENTS.md not found"
            cp /agents/shadowman/agent.json /home/node/.openclaw/workspace/agent.json 2>/dev/null || echo "Warning: shadowman agent.json not found"
          fi

          # Set permissions (for arbitrary UID in OpenShift)
          chmod -R 775 /home/node/.openclaw

          echo "âœ… Basic config initialized with shadowman agent"
          echo "   ðŸ“Š Session transcripts and usage data will be stored in agents/<agent_id>/sessions/"
          echo "   Run agent-setup-job.yaml to add more agents and cron jobs"
          ls -la /home/node/.openclaw/
          ls -la /home/node/.openclaw/workspace/ || true
          ls -la /home/node/.openclaw/agents/ || true
        volumeMounts:
        - name: openclaw-home
          mountPath: /home/node/.openclaw
        - name: config-template
          mountPath: /config
        - name: shadowman-agent
          mountPath: /agents/shadowman
      containers:
      - name: oauth-proxy
        image: quay.io/openshift/origin-oauth-proxy:4.14
        args:
        - --http-address=:8443
        - --https-address=
        - --provider=openshift
        - --upstream=http://localhost:18789
        - --client-id=openclaw
        - --client-secret-file=/etc/oauth/config/client-secret
        - --cookie-secret-file=/etc/oauth/config/cookie_secret
        - --cookie-expire=23h0m0s
        - --pass-access-token
        - --scope=user:full
        - --openshift-delegate-urls={"/":{"resource":"projects","verb":"list"}}
        - --skip-auth-regex=^/(metrics|api)
        ports:
        - containerPort: 8443
          name: oauth-ui
        volumeMounts:
        - name: oauth-config
          mountPath: /etc/oauth/config
        - name: proxy-tls
          mountPath: /etc/tls/private
        livenessProbe:
          httpGet:
            path: /oauth/healthz
            port: oauth-ui
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /oauth/healthz
            port: oauth-ui
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            memory: 64Mi
            cpu: 50m
          limits:
            memory: 128Mi
            cpu: 200m
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      - name: gateway
        image: quay.io/sallyom/openclaw:latest
        imagePullPolicy: Always
        command:
        - node
        - /app/dist/index.js
        - gateway
        - run
        - --bind
        - loopback
        - --port
        - '18789'
        - --verbose
        ports:
        - name: gateway
          containerPort: 18789
          protocol: TCP
        - name: bridge
          containerPort: 18790
          protocol: TCP
        env:
        - name: HOME
          value: /home/node
        - name: OPENCLAW_CONFIG_DIR
          value: /home/node/.openclaw
        - name: OPENCLAW_CLI_PATH
          value: /app/dist/index.js
        - name: TERM
          value: xterm-256color
        - name: NODE_ENV
          value: production
        - name: OPENCLAW_GATEWAY_TOKEN
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: OPENCLAW_GATEWAY_TOKEN
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: ANTHROPIC_API_KEY
              optional: true
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: OTEL_EXPORTER_OTLP_ENDPOINT
              optional: true
        # Telegram bot token (optional â€” only needed for Telegram channel)
        - name: TELEGRAM_BOT_TOKEN
          valueFrom:
            secretKeyRef:
              name: telegram-secrets
              key: bot-token
              optional: true
        # Google Vertex AI (for Gemini or Anthropic models via GCP)
        - name: GOOGLE_APPLICATION_CREDENTIALS
          value: /vertex/ambient-code-key.json
        - name: GCE_METADATA_HOST
          value: disabled
        - name: GOOGLE_CLOUD_PROJECT
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: GOOGLE_CLOUD_PROJECT
              optional: true
        - name: GOOGLE_CLOUD_LOCATION
          valueFrom:
            secretKeyRef:
              name: openclaw-secrets
              key: GOOGLE_CLOUD_LOCATION
              optional: true
        - name: OTEL_SERVICE_NAME
          value: openclaw
        - name: OTEL_EXPORTER_OTLP_PROTOCOL
          value: http/protobuf
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: K8S_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        resources:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 2Gi
            cpu: 1000m
        livenessProbe:
          exec:
            command: ["node", "-e", "require('http').get('http://127.0.0.1:18789/', r => process.exit(r.statusCode < 400 ? 0 : 1)).on('error', () => process.exit(1))"]
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          exec:
            command: ["node", "-e", "require('http').get('http://127.0.0.1:18789/', r => process.exit(r.statusCode < 400 ? 0 : 1)).on('error', () => process.exit(1))"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 2
        volumeMounts:
        - name: openclaw-home
          mountPath: /home/node/.openclaw
        - name: tmp-volume
          mountPath: /tmp
        - name: vertex-credentials
          mountPath: /vertex
          readOnly: true
        - name: resource-report
          mountPath: /data/reports/resource-optimizer
          readOnly: true
        - name: mlops-report
          mountPath: /data/reports/mlops-monitor
          readOnly: true
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      # Agent card server â€” serves /.well-known/agent.json for Kagenti operator discovery.
      # Temporary until Kagenti webhook injects this or operator reads from CR directly.
      - name: agent-card
        image: registry.redhat.io/ubi9:latest
        command: ["python3", "-m", "http.server", "8080", "--directory", "/srv"]
        ports:
        - name: a2a
          containerPort: 8080
        volumeMounts:
        - name: agent-card-data
          mountPath: /srv/.well-known
        resources:
          requests:
            memory: 32Mi
            cpu: 10m
          limits:
            memory: 64Mi
            cpu: 50m
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      # AuthBridge sidecars (spiffe-helper, client-registration, envoy-proxy)
      # are injected by the Kagenti webhook when kagenti.io/inject: enabled.
      volumes:
      - name: tmp-volume
        emptyDir: {}
      # AuthBridge volumes (shared-data, svid-output, spire-agent-socket,
      # spiffe-helper-config, envoy-config) are injected by the Kagenti webhook.
      - name: oauth-config
        secret:
          secretName: openclaw-oauth-config
      - name: proxy-tls
        secret:
          secretName: openclaw-proxy-tls
      - name: openclaw-home
        persistentVolumeClaim:
          claimName: openclaw-home-pvc
      - name: config-template
        configMap:
          name: openclaw-config
      # Google Vertex AI credentials (optional â€” only needed for google-vertex provider)
      - name: vertex-credentials
        secret:
          secretName: ambient-vertex
          optional: true
      # Shadowman agent (included in core deployment)
      - name: shadowman-agent
        configMap:
          name: shadowman-agent
          optional: true
      # Resource report from K8s CronJob (updated every 8 hours)
      - name: resource-report
        configMap:
          name: resource-report-latest
          optional: true
      # MLOps report from K8s CronJob (updated every 6 hours)
      - name: mlops-report
        configMap:
          name: mlops-report-latest
          optional: true
      # Agent card for Kagenti operator discovery (/.well-known/agent.json)
      - name: agent-card-data
        configMap:
          name: openclaw-agent-card
      # Additional agents can be set up via setup-agents.sh
      # All agent workspaces are at ~/.openclaw/workspace* (inside openclaw-home-pvc)
