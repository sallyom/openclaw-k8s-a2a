# Patched npsagent.py — works around two issues with vLLM + Agents SDK:
# 1. MultiProvider strips "openai/" prefix from model names like "openai/gpt-oss-20b"
# 2. vLLM's Responses API doesn't handle function_call_output content lists correctly
# The fix: use OpenAIChatCompletionsModel directly, bypassing both issues.
# This ConfigMap is mounted over /opt/app-root/src/npsagent.py in the container.
# Upstream issue: https://github.com/Nehanth/nps_agent — once vLLM's Responses
# API matures or the upstream fixes model name handling, this patch can be removed.
apiVersion: v1
kind: ConfigMap
metadata:
  name: npsagent-patch
  labels:
    app: nps-agent
    app.kubernetes.io/component: patch
data:
  npsagent.py: |
    import asyncio
    import os

    import nest_asyncio
    nest_asyncio.apply()

    from dotenv import load_dotenv
    load_dotenv()

    import mlflow
    from mlflow.models import set_model
    from mlflow.pyfunc import ResponsesAgent
    from mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentResponse

    from openai import AsyncOpenAI
    from agents import Agent, Runner
    from agents.models.openai_chatcompletions import OpenAIChatCompletionsModel
    from agents.mcp import MCPServerStdio

    # ---------------------------------------------------------------------------
    # Create an NPS Agent  (same pattern as 1_develop/2_evaluate.ipynb)
    # ---------------------------------------------------------------------------
    AGENT_INSTRUCTIONS = (
        "You are a helpful National Parks Service assistant. "
        "Use the available tools to answer questions about national parks, "
        "events, activities, campgrounds, and visitor information. "
    )


    async def run_nps_agent(prompt) -> str:
        """Run the NPS agent with MCP tools and return the text response."""
        command = "uv"
        args = ["run", "fastmcp", "run", "./nps_mcp_server.py"]
        env = {**os.environ, "NPS_API_KEY": os.environ.get("NPS_API_KEY", "")}
        async with MCPServerStdio(params={"command": command, "args": args, "env": env}) as mcp_server:
            # Configure OpenAI-compatible endpoint
            async_client = AsyncOpenAI(
                base_url=os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1"),
                api_key=os.environ.get("OPENAI_API_KEY", ""),
            )

            # Use ChatCompletions model directly to bypass two issues:
            # 1. MultiProvider splits "openai/gpt-oss-20b" into provider="openai" +
            #    model="gpt-oss-20b", losing the full model name that vLLM requires.
            # 2. vLLM's Responses API doesn't handle function_call_output content
            #    lists correctly, causing 400 errors on tool-use follow-up calls.
            model_name = os.environ.get("OPENAI_MODEL_NAME", "gpt-4o")
            model = OpenAIChatCompletionsModel(
                model=model_name,
                openai_client=async_client,
            )

            # Create the agent
            agent = Agent(
                name="NPS Agent",
                instructions=AGENT_INSTRUCTIONS,
                mcp_servers=[mcp_server],
                model=model,
            )

            # Run the agent
            result = await Runner.run(agent, prompt)
            return result.final_output


    # ---------------------------------------------------------------------------
    # MLflow ResponsesAgent — wraps run_nps_agent into an HTTP API for deployment
    # ---------------------------------------------------------------------------
    class NPSResponsesAgent(ResponsesAgent):
        def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
            try:
                result = asyncio.run(run_nps_agent(request.input))
            except Exception as e:
                result = f"Error: {e}"
            return ResponsesAgentResponse(
                output=[self.create_text_output_item(text=result, id="msg_1")]
            )

    # ---------------------------------------------------------------------------
    # MLflow model registration
    # ---------------------------------------------------------------------------
    mlflow.openai.autolog()
    set_model(NPSResponsesAgent())
